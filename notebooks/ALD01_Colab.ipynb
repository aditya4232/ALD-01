{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb568543",
   "metadata": {},
   "source": [
    "# ALD-01 — TinyLlama 1.1B QLoRA Fine-tuning\n",
    "**Indian Bilingual LLM | English + Hindi | Colab Notebook**\n",
    "\n",
    "---\n",
    "Pipeline: Install → Dataset → 4-bit Load → LoRA → Train → Merge → GGUF → Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d102e87",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f01ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "%pip install -q transformers datasets peft bitsandbytes trl accelerate sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff9729e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: clone your GitHub repo\n",
    "# !git clone https://github.com/YOUR_USERNAME/ALD-01-LLM.git\n",
    "# %cd ALD-01-LLM\n",
    "\n",
    "import os\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "os.makedirs(\"outputs/ald01-lora\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0145a4",
   "metadata": {},
   "source": [
    "## 2. Load and Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e92eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "DATASET_PATH = \"data/ald01_dataset.json\"\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=DATASET_PATH, split=\"train\")\n",
    "\n",
    "def format_prompt(sample):\n",
    "    instruction = sample[\"instruction\"]\n",
    "    inp         = sample.get(\"input\", \"\")\n",
    "    output      = sample[\"output\"]\n",
    "    if inp:\n",
    "        text = f\"### Instruction:\\n{instruction}\\n\\n### Input:\\n{inp}\\n\\n### Response:\\n{output}\"\n",
    "    else:\n",
    "        text = f\"### Instruction:\\n{instruction}\\n\\n### Response:\\n{output}\"\n",
    "    return {\"text\": text}\n",
    "\n",
    "dataset = dataset.map(format_prompt)\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "print(dataset[0][\"text\"][:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cffff3",
   "metadata": {},
   "source": [
    "## 3. Load Base Model in 4-bit Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e5c422",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "BASE_MODEL = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit              = True,\n",
    "    bnb_4bit_quant_type       = \"nf4\",\n",
    "    bnb_4bit_compute_dtype    = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16,\n",
    "    bnb_4bit_use_double_quant = True,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)\n",
    "tokenizer.pad_token    = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    quantization_config = bnb_config,\n",
    "    device_map          = \"auto\",\n",
    "    trust_remote_code   = True,\n",
    ")\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "print(\"Model loaded in 4-bit.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb28992",
   "metadata": {},
   "source": [
    "## 4. Configure LoRA Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b64b2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r              = 16,\n",
    "    lora_alpha     = 32,\n",
    "    lora_dropout   = 0.05,\n",
    "    target_modules = [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    bias           = \"none\",\n",
    "    task_type      = \"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

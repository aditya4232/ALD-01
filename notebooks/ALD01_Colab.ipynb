{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb568543",
   "metadata": {},
   "source": [
    "# ALD-01 — TinyLlama 1.1B QLoRA Fine-tuning\n",
    "**Indian Bilingual LLM | English + Hindi | Colab Notebook**\n",
    "\n",
    "---\n",
    "Pipeline: Install → Dataset → 4-bit Load → LoRA → Train → Merge → GGUF → Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d102e87",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f01ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "%pip install -q transformers datasets peft bitsandbytes trl accelerate sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff9729e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: clone your GitHub repo\n",
    "# !git clone https://github.com/YOUR_USERNAME/ALD-01-LLM.git\n",
    "# %cd ALD-01-LLM\n",
    "\n",
    "import os\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "os.makedirs(\"outputs/ald01-lora\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0145a4",
   "metadata": {},
   "source": [
    "## 2. Load and Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e92eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "DATASET_PATH = \"data/ald01_dataset.json\"\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=DATASET_PATH, split=\"train\")\n",
    "\n",
    "def format_prompt(sample):\n",
    "    instruction = sample[\"instruction\"]\n",
    "    inp         = sample.get(\"input\", \"\")\n",
    "    output      = sample[\"output\"]\n",
    "    if inp:\n",
    "        text = f\"### Instruction:\\n{instruction}\\n\\n### Input:\\n{inp}\\n\\n### Response:\\n{output}\"\n",
    "    else:\n",
    "        text = f\"### Instruction:\\n{instruction}\\n\\n### Response:\\n{output}\"\n",
    "    return {\"text\": text}\n",
    "\n",
    "dataset = dataset.map(format_prompt)\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "print(dataset[0][\"text\"][:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cffff3",
   "metadata": {},
   "source": [
    "## 3. Load Base Model in 4-bit Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e5c422",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "BASE_MODEL = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit              = True,\n",
    "    bnb_4bit_quant_type       = \"nf4\",\n",
    "    bnb_4bit_compute_dtype    = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16,\n",
    "    bnb_4bit_use_double_quant = True,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)\n",
    "tokenizer.pad_token    = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    quantization_config = bnb_config,\n",
    "    device_map          = \"auto\",\n",
    "    trust_remote_code   = True,\n",
    ")\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "print(\"Model loaded in 4-bit.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb28992",
   "metadata": {},
   "source": [
    "## 4. Configure LoRA Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b64b2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r              = 16,\n",
    "    lora_alpha     = 32,\n",
    "    lora_dropout   = 0.05,\n",
    "    target_modules = [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    bias           = \"none\",\n",
    "    task_type      = \"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc519ed",
   "metadata": {},
   "source": [
    "## 5. Configure Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4ac7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "OUTPUT_DIR = \"outputs/ald01-lora\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir                  = OUTPUT_DIR,\n",
    "    num_train_epochs            = 3,\n",
    "    per_device_train_batch_size = 2,\n",
    "    gradient_accumulation_steps = 4,\n",
    "    gradient_checkpointing      = True,\n",
    "    learning_rate               = 2e-4,\n",
    "    lr_scheduler_type           = \"cosine\",\n",
    "    warmup_ratio                = 0.05,\n",
    "    logging_steps               = 10,\n",
    "    save_steps                  = 100,\n",
    "    save_total_limit            = 2,\n",
    "    fp16                        = not torch.cuda.is_bf16_supported(),\n",
    "    bf16                        = torch.cuda.is_bf16_supported(),\n",
    "    optim                       = \"paged_adamw_8bit\",\n",
    "    report_to                   = \"none\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb618b16",
   "metadata": {},
   "source": [
    "## 6. Run QLoRA Fine-Tuning with SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de6de3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model              = model,\n",
    "    train_dataset      = dataset,\n",
    "    tokenizer          = tokenizer,\n",
    "    args               = training_args,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length     = 1024,\n",
    "    packing            = False,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9160f158",
   "metadata": {},
   "source": [
    "## 7. Save LoRA Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c51d4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.save_pretrained(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "print(f\"LoRA adapter saved to: {OUTPUT_DIR}\")\n",
    "\n",
    "# Save to Google Drive (uncomment in Colab)\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# !cp -r {OUTPUT_DIR} \"/content/drive/MyDrive/ALD-01/\"\n",
    "# print(\"Adapter copied to Google Drive.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4523a4ce",
   "metadata": {},
   "source": [
    "## 8. Merge LoRA Adapter into Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cb20f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "MERGED_DIR = \"outputs/ald01-merged\"\n",
    "\n",
    "merge_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    torch_dtype       = torch.float16,\n",
    "    device_map        = \"cpu\",\n",
    "    trust_remote_code = True,\n",
    ")\n",
    "merge_model = PeftModel.from_pretrained(merge_model, OUTPUT_DIR)\n",
    "merge_model = merge_model.merge_and_unload()\n",
    "\n",
    "merge_model.save_pretrained(MERGED_DIR, safe_serialization=True)\n",
    "tokenizer.save_pretrained(MERGED_DIR)\n",
    "print(f\"Merged model saved to: {MERGED_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415d6b24",
   "metadata": {},
   "source": [
    "## 9. Convert to GGUF Format (Q4_K_M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce19eb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Clone and build llama.cpp\n",
    "git clone --depth 1 https://github.com/ggerganov/llama.cpp /content/llama.cpp\n",
    "pip install -q -r /content/llama.cpp/requirements.txt\n",
    "make -C /content/llama.cpp -j2 llama-quantize\n",
    "\n",
    "# Step 1: Convert HF → GGUF (f16)\n",
    "python /content/llama.cpp/convert_hf_to_gguf.py outputs/ald01-merged \\\n",
    "    --outfile outputs/ald01-f16.gguf \\\n",
    "    --outtype f16\n",
    "\n",
    "# Step 2: Quantize → Q4_K_M (~650 MB)\n",
    "/content/llama.cpp/llama-quantize \\\n",
    "    outputs/ald01-f16.gguf \\\n",
    "    outputs/ald01-Q4_K_M.gguf \\\n",
    "    Q4_K_M\n",
    "\n",
    "echo \"GGUF quantization complete.\"\n",
    "ls -lh outputs/*.gguf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf00903",
   "metadata": {},
   "source": [
    "## 10. Evaluate ALD-01 Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe1229b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline as hf_pipeline\n",
    "\n",
    "eval_pipe = hf_pipeline(\n",
    "    \"text-generation\",\n",
    "    model              = MERGED_DIR,\n",
    "    torch_dtype        = torch.float16,\n",
    "    device_map         = \"auto\",\n",
    "    max_new_tokens     = 256,\n",
    "    do_sample          = True,\n",
    "    temperature        = 0.7,\n",
    "    repetition_penalty = 1.1,\n",
    ")\n",
    "\n",
    "TEST_CASES = [\n",
    "    (\"Hindi Response\",        \"Machine learning ko simple Hindi mein explain karo.\"),\n",
    "    (\"Office Email\",          \"Write a professional email to inform a client about a 1-week project delay due to server migration.\"),\n",
    "    (\"React Component\",       \"Write a minimal React functional component that displays a loading spinner.\"),\n",
    "    (\"Logical Reasoning\",     \"If all A are B, and all B are C, are all A necessarily C? Explain step by step.\"),\n",
    "]\n",
    "\n",
    "for label, instruction in TEST_CASES:\n",
    "    prompt = f\"### Instruction:\\n{instruction}\\n\\n### Response:\\n\"\n",
    "    out    = eval_pipe(prompt)[0][\"generated_text\"]\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"[{label}]\")\n",
    "    print(\"-\" * 40)\n",
    "    print(out[len(prompt):].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01853554",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "\n",
    "| Error | Fix |\n",
    "|---|---|\n",
    "| `CUDA out of memory` | Reduce `per_device_train_batch_size` to 1, increase `gradient_accumulation_steps` to 8 |\n",
    "| `bitsandbytes not available` | Run: `!pip install bitsandbytes --upgrade` and restart runtime |\n",
    "| `ModuleNotFoundError: trl` | Run: `!pip install trl --upgrade` and restart |\n",
    "| Model outputs gibberish | Check tokenizer `pad_token` is set to `eos_token` |\n",
    "| Training loss not decreasing | Increase `num_train_epochs`, check dataset format (instruction/output fields) |\n",
    "| Colab session crashes | Enable GPU: Runtime → Change Runtime Type → T4 GPU |"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
